{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAlex\n",
    "OpenAlex seems to be more promising than SemanticScholar at least because we can download its dataset. But let's do things consecutively. \n",
    "\n",
    "We are given: \n",
    "- pid of researcher from dblp.org\n",
    "- list doi of papers by given researcher\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Rate Limits\n",
    "1. Requests per second:\tUp to ~1 request per second recommended\n",
    "2. Burst limit:\tShort bursts slightly above 1 rps allowed\n",
    "3. Requests per day:\tNot officially documented â€” flexible within reason\n",
    "4. Pagination limit:\tMax 200 results per page (default 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pub data using dblp pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OPENALEX_API_BASE = \"https://api.openalex.org\"\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "\n",
    "# Adjustable parameters\n",
    "RATE_LIMIT_DELAY = 1  # seconds between requests (for polite use)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to query author: 403\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Get Author Publications via DBLP ID\n",
    "# ----------------------------\n",
    "def get_author_id_by_dblp(dblp_id: str):\n",
    "    url = f\"{OPENALEX_API_BASE}/authors?filter=ids.dblp:{dblp_id}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", [])\n",
    "        if results:\n",
    "            author = results[0]  # Take first match\n",
    "            print(f\"Found author: {author['display_name']} (OpenAlex ID: {author['id']})\")\n",
    "            return author['id']\n",
    "        else:\n",
    "            print(\"No author found with given DBLP ID.\")\n",
    "    else:\n",
    "        print(f\"Failed to query author: {response.status_code}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_author_works(author_id: str, max_results=50):\n",
    "    works = []\n",
    "    page = 1\n",
    "    per_page = 25  # Max 200, but let's start small to avoid rate limits\n",
    "    print(f\"Fetching works for author {author_id}...\")\n",
    "    \n",
    "    while len(works) < max_results:\n",
    "        url = f\"{OPENALEX_API_BASE}/works?filter=authorships.author.id:{author_id}&per-page={per_page}&page={page}\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            page_results = response.json().get(\"results\", [])\n",
    "            if not page_results:\n",
    "                break  # No more results\n",
    "            works.extend(page_results)\n",
    "            print(f\"Fetched {len(works)} works so far...\")\n",
    "            page += 1\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "        else:\n",
    "            print(f\"Failed to fetch works: {response.status_code}\")\n",
    "            break\n",
    "    return works[:max_results]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example 1: Author publications via DBLP ID\n",
    "dblp_id = \"https://dblp.org/pid/32/5357\"  # Replace with actual DBLP ID\n",
    "author_id = get_author_id_by_dblp(dblp_id)\n",
    "if author_id:\n",
    "    author_works = get_author_works(author_id, max_results=20)  # Limit to 20 works\n",
    "    with open(\"author_publications.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(author_works, f, indent=4)\n",
    "    print(f\"Saved {len(author_works)} works to 'author_publications.json'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pub data via DOI\n",
    "\n",
    "Works well. We get citation counts and an abstract. \n",
    "\n",
    "**Problem**: For polite API usage we have to wait 1sec between API calls which could result in hunderds of seconds wait time. Also The returned data is quite large but it's least of our issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata for 2 papers to 'papers_metadata.json'.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2. Get Publication Data via DOIs\n",
    "# ----------------------------\n",
    "def get_work_by_doi(doi: str):\n",
    "    url = f\"{OPENALEX_API_BASE}/works/https://doi.org/{doi}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for DOI {doi}: {response.status_code}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_bulk_paper_data(dois: list):\n",
    "    papers_data = []\n",
    "    for doi in dois:\n",
    "        paper = get_work_by_doi(doi)\n",
    "        if paper:\n",
    "            papers_data.append(paper)\n",
    "        time.sleep(RATE_LIMIT_DELAY)  # Respect polite API usage\n",
    "    return papers_data\n",
    "\n",
    "\n",
    "# Example 2: Bulk paper data via DOIs\n",
    "dois = [\n",
    "    \"10.1145/3366423.3380250\",\n",
    "    \"10.1038/s41586-020-2649-2\"\n",
    "    # Add more DOIs as needed\n",
    "]\n",
    "papers_data = fetch_bulk_paper_data(dois)\n",
    "with open(\"papers_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(papers_data, f, indent=4)\n",
    "print(f\"Saved metadata for {len(papers_data)} papers to 'papers_metadata.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data in chunks\n",
    "Check asyn_fetch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading OpenAlex dataset\n",
    "\n",
    "\n",
    "\n",
    "Reference here: https://registry.opendata.aws/openalex/\n",
    "\n",
    "https://docs.openalex.org/download-all-data/openalex-snapshot\n",
    "\n",
    "Schema: https://github.com/ourresearch/openalex-documentation-scripts/blob/main/openalex-pg-schema.sql\n",
    "\n",
    "Abstracts data takes 412 GB of space, which is BANANAS and it's compressed. So uncompressed would be more than a 1 TB.  So let's stick to querying data\n",
    "\n",
    "https://openalex.s3.amazonaws.com/browse.html#data/works/updated_date=2024-10-02/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
